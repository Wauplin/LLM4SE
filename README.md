# Large Language Model for Software Engineering

# Table of Contents

| [Paper List](#paper-list) |
| [Paper Stats](#paper-stats) |

<a name="paper-list"></a>
## Paper List

<details>
<summary>Click to expand!</summary>
  
| Year-Id | Title        | Venue Name(Type) |
|---------|---------------------------------------------------------------------------------------------------------------------------------|------------|
| 2023-1  | [Invalidator: Automated Patch Correctness Assessment via Semantic and Syntactic Reasoning](https://10.1109/TSE.2023.3255177)              | TSE(J)        |
| 2022-20 | [Automating code review activities by large-scale pre-training](https://dl.acm.org/doi/10.1145/3540250.3549081)                                                                                    | FSE(C)        |
| 2022-19 | [VulCurator: A Vulnerability-fixing Commit Detector](https://doi.org/10.1145/3540250.3558936)       | FSE(C)        |
| 2022-18 | [AutoPruner: Transformer-based Call Graph Pruning](https://doi.org/10.1145/3540250.3549175)       | FSE(C)        |
| 2022-17 | [Can pre-trained code embeddings improve model performance? Revisiting the use of code embeddings in software engineering tasks](https://doi.org/10.1007/s10664-022-10118-5)    | EMSE(J)       |
| 2022-16 | [Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding](https://doi.org/10.1145/3510003.3510062)       | ICSE(C)       |
| 2022-15 | [Jigsaw: Large Language Models meet Program Synthesis](https://doi.org/10.1145/3510003.3510203)       | ICSE(C)       |
| 2022-14 | [Natural Attack for Pre-trained Models of Code](https://doi.org/10.1145/3510003.3510146)       | ICSE(C)       |
| 2022-13 | [Using Pre-Trained Models to Boost Code Review Automation](https://doi.org/10.1145/3510003.3510621)       | ICSE(C)       |
| 2022-12 | [What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source Code](https://doi.org/10.1145/3510003.3510050)       | ICSE(C)       |
| 2022-11 | [A Light Bug Triage Framework for Applying Large Pre-trained Language Model](https://doi.org/10.1145/3551349.3556898)       | ASE(C)        |
| 2022-10 | [AST-Probe: Recovering abstract syntax trees from hidden representations of pre-trained language models](https://doi.org/10.1145/3551349.3556900)       | ASE(C)        |
| 2022-9  | [Compressing Pre-trained Models of Code into 3 MB](https://doi.org/10.1145/3551349.3556964)       | ASE(C)        |
| 2022-8  | [PRCBERT: Prompt Learning for Requirement Classification using BERT-based Pretrained Language Models](https://doi.org/10.1145/3551349.3560417)       | ASE(C)        |
| 2022-7  | [Prompt-tuned Code Language Model as a Neural Knowledge Base for Type Inference in Statically-Typed Partial Code](https://doi.org/10.1145/3551349.3556912)       | ASE(C)        |
| 2022-6  | [Few-shot training LLMs for project-specific code-summarization](https://doi.org/10.1145/3551349.3559555)       | ASE(C)        |
| 2022-5  | [Diet code is healthy: simplifying programs for pre-trained models of code](https://doi.org/10.1145/3540250.3549094)       | FSE(C)        |
| 2022-4  | [Discrepancies among pre-trained deep neural networks: a new threat to model zoo reliability](https://doi.org/10.1145/3540250.3560881)       | FSE(C)        |
| 2022-3  | [Effective and scalable fault injection using bug reports and generative language models](https://doi.org/10.1145/3540250.3558907)       | FSE(C)        |
| 2022-2  | [An extensive study on pre-trained models for program understanding and generation](https://doi.org/10.1145/3533767.3534390)       | ISSTA(C)      |
| 2022-1  | [Using pre-trained language models to resolve textual and semantic merge conflicts (experience paper)](https://doi.org/10.1145/3533767.3534396)       | ISSTA(C)      |
| 2021-4  | [Traceability Transformed: Generating more Accurate Links with Pre-Trained BERT Models](https://doi.org/10.1109/ICSE43902.2021.00040)  | ICSE(C)       |
| 2021-3  | [DeepMemory: Model-based Memorization Analysis of Deep Neural Language Models](https://doi.org/10.1109/ASE51524.2021.9678871) | ASE(C)        |
| 2021-2  | [What do pre-trained code models know about code?](https://doi.org/10.1109/ASE51524.2021.9678927) | ASE(C)        |
| 2021-1  | [Does reusing pre-trained NLP model propagate bugs?](https://doi.org/10.1145/3468264.3473494)       | FSE(C)        |
| 2020-1  | [Multi-task Learning based Pre-trained Language Model for Code Completion](https://doi.org/10.1145/3324884.3416591)       | ASE(C)        |
  
</details>

<a name="paper-stats"></a>
## Paper Stats


### Venue Stats

| Venue | Count |
|-------|-------|
| ICSE  |   7   |
| FSE   |   7    |
| ASE   |   9    |
| ISSTA |   2    |
| TSE   |   1    |
| TOSEM |   0    |
| EMSE  |    1   |

### Year Stats

| Venue | Count |
|-------|-------|
| 2023  |   1    |
| 2022  |   20    |
| 2021   |   4    |
| 2020 |    1   |

## Considered Venues
Powered by an automation tool, mainteners routinary check for new program repair papers that appear in the venues below:

### Conferences
- **ICSE**: International Conference on Software Engineering
- **FSE**: The ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering
- **ASE**: IEEE/ACM International Conference on Automated Software Engineering
- **ISSTA**: International Symposium on Software Testing and Analysis

### Journals
- **TSE**: IEEE Transactions on Software Engineering
- **TOSEM**: ACM Transactions on Software Engineering and Methodology
- **EMSE**: Empirical Software Engineering

### Contribution
The easiest way to contribute is to submit a paper with verified information via GitHub issues. Only url of the paper should be already enough. The mainteiner will add accordingly and keep you updated in the issue conversation.

Alternatively, you can create a pull request. For that, you need to strictly follow the format.

Any other suggestion to improve this repository is also highly welcomed via GitHub issues.
